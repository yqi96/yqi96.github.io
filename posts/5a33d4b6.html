<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>多卡训练时非tensor输入 | BLINGING</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">多卡训练时非tensor输入</h1><a id="logo" href="/.">BLINGING</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">多卡训练时非tensor输入</h1><div class="post-meta">2021-01-18<span> | </span><span class="category"><a href="/categories/%E4%BB%A3%E7%A0%81%E5%9D%97/">代码块</a></span></div><div class="post-content"><p>　　DataParallel类的forward中通过scatter方法把输入分配到各显卡，并行计算后通过gather方法进行整合。scatter只实现了对torch.Tensor的按batch维度切分，tuple、list、dict等类则进行递归，其他类型保持不变。</p>
<a id="more"></a>
<p>　　list：[Tensor1, Tensor2]作为输入，2卡训练，scatter将Tensor1和Tensor2分别切分成2个：Tensor1_a、Tensor1_b、Tensor2_a、Tensor2_b，并把[Tensor1_a, Tensor2_a]作为GPU1的输入，[Tensor1_b, Tensor2_b]作为GPU2的输入。</p>
<p>　　list：[ndarray1, ndarray2]作为输入，2卡训练时，由于该list内的元素都不是Tensor，因此GPU1和GPU2都接受完整的[ndarray1, ndarray2]作为输入。</p>
<p>　　Pytorch 1.7的scatter的具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter</span>(<span class="params">inputs, target_gpus, dim=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Slices tensors into approximately equal chunks and</span></span><br><span class="line"><span class="string">    distributes them across given GPUs. Duplicates</span></span><br><span class="line"><span class="string">    references to objects that are not tensors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scatter_map</span>(<span class="params">obj</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, torch.Tensor):</span><br><span class="line">            <span class="keyword">return</span> Scatter.apply(target_gpus, <span class="literal">None</span>, dim, obj)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj)))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>, <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj))))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">dict</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">type</span>(obj), <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj.items()))))</span><br><span class="line">        <span class="keyword">return</span> [obj <span class="keyword">for</span> targets <span class="keyword">in</span> target_gpus]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># After scatter_map is called, a scatter_map cell will exist. This cell</span></span><br><span class="line">    <span class="comment"># has a reference to the actual function scatter_map, which has references</span></span><br><span class="line">    <span class="comment"># to a closure that has a reference to the scatter_map cell (because the</span></span><br><span class="line">    <span class="comment"># fn is recursive). To avoid this reference cycle, we set the function to</span></span><br><span class="line">    <span class="comment"># None, clearing the cell</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = scatter_map(inputs)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        scatter_map = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>　　某些情况下可能需要以长度为batchsize的list作为输入。以2卡为例，希望把list的前半分配给GPU1，后半分配给GPU2，这是无法用DataParalle来实现的。采取策略为：list内的元素不用Tensor，可以用np.ndarray或其他类型，这样每张卡上的模型都会接受到完整的list。另外设置一个torch.Tensor类的index变量作为输入，scatter会将index切分成两份。在模型的forward中利用index来获取对应的list切片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>():</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x, index</span>):</span></span><br><span class="line">        x = x[<span class="built_in">int</span>(index[<span class="number">0</span>]) : <span class="built_in">int</span>(index[-<span class="number">1</span>]) + <span class="number">1</span>]</span><br><span class="line">        x = [torch.from_numpy(i).to(index.device) <span class="keyword">for</span> i <span class="keyword">in</span> inputs]</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">model = nn.DataParalle(Model())</span><br><span class="line">index = torch.tensor(<span class="built_in">list</span>(<span class="built_in">range</span>(batch_size))).to(device)</span><br><span class="line">x = [ndarray1, ndarray2, ..., ndarray16]</span><br><span class="line">out = model(x, index)</span><br></pre></td></tr></table></figure>


</div><div class="tags"><a href="/tags/Pytorch/"><i class="fa fa-tag"></i>Pytorch</a></div><div class="post-nav"><a class="pre" href="/posts/9bf2951e.html">InceptionConv</a><a class="next" href="/posts/52f897dd.html">DataLoader无限循环</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Note/">Note</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BB%A3%E7%A0%81%E5%9D%97/">代码块</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/fusion/" style="font-size: 15px;">fusion</a> <a href="/tags/NeurIPS/" style="font-size: 15px;">NeurIPS</a> <a href="/tags/CVPR/" style="font-size: 15px;">CVPR</a> <a href="/tags/loss/" style="font-size: 15px;">loss</a> <a href="/tags/detection/" style="font-size: 15px;">detection</a> <a href="/tags/light-weight/" style="font-size: 15px;">light-weight</a> <a href="/tags/two-stage/" style="font-size: 15px;">two-stage</a> <a href="/tags/ICCV/" style="font-size: 15px;">ICCV</a> <a href="/tags/upsample/" style="font-size: 15px;">upsample</a> <a href="/tags/backbone/" style="font-size: 15px;">backbone</a> <a href="/tags/cascade/" style="font-size: 15px;">cascade</a> <a href="/tags/aerial/" style="font-size: 15px;">aerial</a> <a href="/tags/ECCV/" style="font-size: 15px;">ECCV</a> <a href="/tags/classification/" style="font-size: 15px;">classification</a> <a href="/tags/attribute/" style="font-size: 15px;">attribute</a> <a href="/tags/attention/" style="font-size: 15px;">attention</a> <a href="/tags/Visual-Cognition/" style="font-size: 15px;">Visual Cognition</a> <a href="/tags/neck/" style="font-size: 15px;">neck</a> <a href="/tags/ICML/" style="font-size: 15px;">ICML</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/scratch/" style="font-size: 15px;">scratch</a> <a href="/tags/gradient/" style="font-size: 15px;">gradient</a> <a href="/tags/relationship/" style="font-size: 15px;">relationship</a> <a href="/tags/scale/" style="font-size: 15px;">scale</a> <a href="/tags/dataset/" style="font-size: 15px;">dataset</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/posts/9bf2951e.html">InceptionConv</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/5a33d4b6.html">多卡训练时非tensor输入</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/52f897dd.html">DataLoader无限循环</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/6deaeeb1.html">Softmax和Cross-entropy的数值稳定问题</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/1b0927de.html">Hexo 安装配置</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/b956e9d5.html">CSPNet</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/a510d887.html">EfficientDet</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/ed54d5da.html">CARAFE</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/e77010ad.html">ClusDet</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/102b1f4c.html">DAFS</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">BLINGING.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>