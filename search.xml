<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>多卡训练时非tensor输入</title>
      <link href="posts/5a33d4b6.html"/>
      <url>posts/5a33d4b6.html</url>
      
        <content type="html"><![CDATA[<p>　　DataParallel类的forward中通过scatter方法把输入分配到各显卡，并行计算后通过gather方法进行整合。scatter只实现了对torch.Tensor的按batch维度切分，tuple、list、dict等类则进行递归，其他类型保持不变。</p><a id="more"></a><p>　　list：[Tensor1, Tensor2]作为输入，2卡训练，scatter将Tensor1和Tensor2分别切分成2个：Tensor1_a、Tensor1_b、Tensor2_a、Tensor2_b，并把[Tensor1_a, Tensor2_a]作为GPU1的输入，[Tensor1_b, Tensor2_b]作为GPU2的输入。</p><p>　　list：[ndarray1, ndarray2]作为输入，2卡训练时，由于该list内的元素都不是Tensor，因此GPU1和GPU2都接受完整的[ndarray1, ndarray2]作为输入。</p><p>　　Pytorch 1.7的scatter的具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter</span>(<span class="params">inputs, target_gpus, dim=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Slices tensors into approximately equal chunks and</span></span><br><span class="line"><span class="string">    distributes them across given GPUs. Duplicates</span></span><br><span class="line"><span class="string">    references to objects that are not tensors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scatter_map</span>(<span class="params">obj</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, torch.Tensor):</span><br><span class="line">            <span class="keyword">return</span> Scatter.apply(target_gpus, <span class="literal">None</span>, dim, obj)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj)))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>, <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj))))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">dict</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">type</span>(obj), <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj.items()))))</span><br><span class="line">        <span class="keyword">return</span> [obj <span class="keyword">for</span> targets <span class="keyword">in</span> target_gpus]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># After scatter_map is called, a scatter_map cell will exist. This cell</span></span><br><span class="line">    <span class="comment"># has a reference to the actual function scatter_map, which has references</span></span><br><span class="line">    <span class="comment"># to a closure that has a reference to the scatter_map cell (because the</span></span><br><span class="line">    <span class="comment"># fn is recursive). To avoid this reference cycle, we set the function to</span></span><br><span class="line">    <span class="comment"># None, clearing the cell</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = scatter_map(inputs)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        scatter_map = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>　　某些情况下可能需要以长度为batchsize的list作为输入。以2卡为例，希望把list的前半分配给GPU1，后半分配给GPU2，这是无法用DataParalle来实现的。采取策略为：list内的元素不用Tensor，可以用np.ndarray或其他类型，这样每张卡上的模型都会接受到完整的list。另外设置一个torch.Tensor类的index变量作为输入，scatter会将index切分成两份。在模型的forward中利用index来获取对应的list切片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>():</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x, index</span>):</span></span><br><span class="line">        x = x[<span class="built_in">int</span>(index[<span class="number">0</span>]) : <span class="built_in">int</span>(index[-<span class="number">1</span>]) + <span class="number">1</span>]</span><br><span class="line">        x = [torch.from_numpy(i).to(index.device) <span class="keyword">for</span> i <span class="keyword">in</span> inputs]</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">model = nn.DataParalle(Model())</span><br><span class="line">index = torch.tensor(<span class="built_in">list</span>(<span class="built_in">range</span>(batch_size))).to(device)</span><br><span class="line">x = [ndarray1, ndarray2, ..., ndarray16]</span><br><span class="line">out = model(x, index)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 代码块 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataLoader无限循环</title>
      <link href="posts/52f897dd.html"/>
      <url>posts/52f897dd.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inf_gen</span>(<span class="params">dataloader</span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> dataloader:</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 代码块 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Softmax和Cross-entropy的数值稳定问题</title>
      <link href="posts/6deaeeb1.html"/>
      <url>posts/6deaeeb1.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } },  "HTML-CSS": { linebreaks: { automatic: true } },         SVG: { linebreaks: { automatic: true } }});</script><a id="more"></a><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>Softmax公式:<br>$$<br>\begin{equation}<br>p_i=\frac{e^{l_i}}{\sum_\limits{j}e^{l_j}}<br>\end{equation}<br>$$</p><p>当logits太大时，指数求和部分会出现上溢，解决方案：</p><p>$$<br>\begin{equation}<br>p_i=\frac{e^{l_i-l_{max}}}{\sum_\limits{j}e^{l_j-l_{max}}}<br>\end{equation}<br>$$</p><p>\(l_j-l_{max}\le0\)且必有至少一项为0，因此指数和不会出现上溢，分母不会出现下溢。</p><p>Cross-entropy公式：<br>$$<br>\begin{equation}<br>xent=-\sum_\limits{i}y_i\ln p_i<br>\end{equation}<br>$$<br>若\(p_i\)接近0，则\(\ln p_i\)会发生下溢，解决方案：</p><p>将(2)代入(3),有</p><p>$$<br>\begin{equation}<br>\begin{aligned}<br>xent&amp;=-\sum_\limits{i}y_i\ln {(\frac{e^{l_i-l_{max}}}{\sum_\limits{j}e^{l_j-l_{max}}})} \\<br>&amp;=-\sum_\limits{i}y_i(l_i-l_{max}-\ln{\sum_\limits{j}e^{l_j-l_{max}}})\\<br>&amp;=\sum_\limits{i}y_i(\ln{\sum_\limits{j}e^{l_j-l_{max}}}+l_{max}-l_i)<br>\end{aligned}<br>\end{equation}<br>$$</p><h3 id="Pytorch简单实现"><a href="#Pytorch简单实现" class="headerlink" title="Pytorch简单实现"></a>Pytorch简单实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxCrossEntropyLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim=<span class="number">1</span>, mean=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SoftmaxCrossEntropyLoss, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.mean = mean</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, logits, targets</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(targets.shape) == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># one-hot</span></span><br><span class="line">            ones = torch.eye(logits.shape[self.dim]).to(targets.device)</span><br><span class="line">            targets = ones.index_select(<span class="number">0</span>, targets)</span><br><span class="line">        </span><br><span class="line">        max_logit = logits.<span class="built_in">max</span>()</span><br><span class="line">        exp_sum = (logits - max_logit).exp().<span class="built_in">sum</span>(dim=self.dim, keepdim=<span class="literal">True</span>)</span><br><span class="line">        log_softmax = logits - max_logit - exp_sum.log()</span><br><span class="line"></span><br><span class="line">        xent = - (targets * log_softmax).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.mean:</span><br><span class="line">            xent /= logits.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> xent</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 安装配置</title>
      <link href="posts/1b0927de.html"/>
      <url>posts/1b0927de.html</url>
      
        <content type="html"><![CDATA[<h3 id="基础功能"><a href="#基础功能" class="headerlink" title="基础功能"></a>基础功能</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org</span><br><span class="line">npm install hexo-cli -g</span><br><span class="line">mkdir blog &amp;&amp; <span class="built_in">cd</span> blog</span><br><span class="line">hexo init</span><br><span class="line">npm install hexo-server --save</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</span><br><span class="line">npm install hexo-renderer-pug --save</span><br><span class="line">npm install hexo-renderer-sass --save</span><br></pre></td></tr></table></figure><h3 id="页面显示"><a href="#页面显示" class="headerlink" title="页面显示"></a>页面显示</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-archive --save</span><br><span class="line">npm install hexo-generator-category --save</span><br><span class="line">npm install hexo-generator-tag --save</span><br><span class="line">npm install hexo-generator-search --save</span><br><span class="line"><span class="comment"># npm install hexo-asset-image --save</span></span><br></pre></td></tr></table></figure><h3 id="唯一链接"><a href="#唯一链接" class="headerlink" title="唯一链接"></a>唯一链接</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-abbrlink --save</span><br><span class="line">npm install https://github.com/foreveryang321/hexo-asset-image.git --save</span><br></pre></td></tr></table></figure><h3 id="备忘"><a href="#备忘" class="headerlink" title="备忘"></a>备忘</h3><ul><li>兼容<a href="https://github.com/Rozbo/hexo-abbrlink">hexo-abbrlink</a>的<a href="https://github.com/foreveryang321/hexo-asset-image">hexo-asset-image</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSPNet</title>
      <link href="posts/b956e9d5.html"/>
      <url>posts/b956e9d5.html</url>
      
        <content type="html"><![CDATA[<h3 id="《CSPNet-A-New-Backbone-that-can-Enhance-Learning-Capability-of-CNN》"><a href="#《CSPNet-A-New-Backbone-that-can-Enhance-Learning-Capability-of-CNN》" class="headerlink" title="《CSPNet: A New Backbone that can Enhance Learning Capability of CNN》"></a>《<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.pdf">CSPNet: A New Backbone that can Enhance Learning Capability of CNN</a>》</h3><a id="more"></a><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li>相比PRN，不但增加了梯度组合，还减少了运算</li><li>灵活性更强</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>按通道将特征谱分成两份，其中一份接各种模块后和另一份融合<br><img src="/posts/b956e9d5/CSP-dense.png"><br><img src="/posts/b956e9d5/CSP-res.png"></p><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3></li><li><p><a href="http://blinging.xyz/posts/edc10435.html">SparseNet</a></p></li><li><p><a href="http://blinging.xyz/posts/8a14a4e3.html">PRN</a></p></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientDet</title>
      <link href="posts/a510d887.html"/>
      <url>posts/a510d887.html</url>
      
        <content type="html"><![CDATA[<h3 id="《EfficientDet-Scalable-and-Efficient-Object-Detection》"><a href="#《EfficientDet-Scalable-and-Efficient-Object-Detection》" class="headerlink" title="《EfficientDet: Scalable and Efficient Object Detection》"></a>《<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf">EfficientDet: Scalable and Efficient Object Detection</a>》</h3><a id="more"></a><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li>提出高效的特征融合模块BiFPN<ul><li>去除了只有一个输入的节点</li><li>添加额外的连接</li><li>特征求和部分添加可学习的权重参数</li><li>BiFPN多次叠加构成Neck<br><img src="/posts/a510d887/EfficientDet.png" alt="EfficientDet"></li></ul></li><li>用EfficientNet的思路探索了合适的Neck、Head和输入分辨率配置</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/a8875d51.html">EfficientNet</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fusion </tag>
            
            <tag> CVPR </tag>
            
            <tag> detection </tag>
            
            <tag> neck </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CARAFE</title>
      <link href="posts/ed54d5da.html"/>
      <url>posts/ed54d5da.html</url>
      
        <content type="html"><![CDATA[<h3 id="《CARAFE-Content-Aware-ReAssembly-of-FEatures》"><a href="#《CARAFE-Content-Aware-ReAssembly-of-FEatures》" class="headerlink" title="《CARAFE: Content-Aware ReAssembly of FEatures》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.pdf">CARAFE: Content-Aware ReAssembly of FEatures</a>》</h3><a id="more"></a><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><ul><li>解决检测、分割等任务中的上采样问题</li></ul><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul><li>大感受野</li><li>内容感知</li><li>轻量级</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p><img src="/posts/ed54d5da/CARAFE.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><ul><li><a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> light-weight </tag>
            
            <tag> ICCV </tag>
            
            <tag> upsample </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ClusDet</title>
      <link href="posts/e77010ad.html"/>
      <url>posts/e77010ad.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Clustered-Object-Detection-in-Aerial-Images》"><a href="#《Clustered-Object-Detection-in-Aerial-Images》" class="headerlink" title="《Clustered Object Detection in Aerial Images》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.pdf">Clustered Object Detection in Aerial Images</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li><p>输入分辨率高</p></li><li><p>目标小</p></li><li><p>目标稀疏且非均匀分布</p></li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>检测目标聚集区域，并从该区域内检测，将结果与全局检测结合</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
            <tag> aerial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DAFS</title>
      <link href="posts/102b1f4c.html"/>
      <url>posts/102b1f4c.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Dynamic-Anchor-Feature-Selection-for-Single-Shot-Object-Detection》"><a href="#《Dynamic-Anchor-Feature-Selection-for-Single-Shot-Object-Detection》" class="headerlink" title="《Dynamic Anchor Feature Selection for Single-Shot Object Detection》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf">Dynamic Anchor Feature Selection for Single-Shot Object Detection</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>RefineDet的ODM预测时规则卷积的感受野和refine过的anchor位置不匹配</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>提出Dynamic Anchor Feature Selection(DAFS)，根据refine后的anchor动态地选择特征</li><li>改进RefineDet的TCB模块，双向聚合特征谱</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LRFNet</title>
      <link href="posts/127af3.html"/>
      <url>posts/127af3.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Learning-Rich-Features-at-High-Speed-for-Single-Shot-Object-Detection》"><a href="#《Learning-Rich-Features-at-High-Speed-for-Single-Shot-Object-Detection》" class="headerlink" title="《Learning Rich Features at High-Speed for Single-Shot Object Detection》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf">Learning Rich Features at High-Speed for Single-Shot Object Detection</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>现有多数目标检测网络是从ImageNet预训练的分类网络微调的，但是检测和分类任务对平移的敏感性要求不同，预训练网络和检测任务之间有一定差异，用分类预训练网络不是最优的。但是从头开始训练检测网络需要大量时间。</li></ul><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li>用分类预训练网络做主体，外接一路从头训练的小网络，结合两者特征</li></ul><p><img src="/posts/127af3/LRF.png"></p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
            <tag> scratch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PRN</title>
      <link href="posts/8a14a4e3.html"/>
      <url>posts/8a14a4e3.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Enriching-Variety-of-Layer-wise-Learning-Information-by-Gradient-Combination》"><a href="#《Enriching-Variety-of-Layer-wise-Learning-Information-by-Gradient-Combination》" class="headerlink" title="《Enriching Variety of Layer-wise Learning Information by Gradient Combination》"></a>《<a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Wang_Enriching_Variety_of_Layer-Wise_Learning_Information_by_Gradient_Combination_ICCVW_2019_paper.pdf">Enriching Variety of Layer-wise Learning Information by Gradient Combination</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>之前的文献关注如何组合特征和如何使梯度得以传播，但是否可以通过得到更丰富的梯度组合来提高网络性能？</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>部分通道求和（下图中的特征谱按对应颜色的箭头进行正向传播）<br><img src="/posts/8a14a4e3/PRN-shrink.png" alt="PRN-shrink"><br><img src="/posts/8a14a4e3/PRN-expand.png" alt="PRN-expand"><br><img src="/posts/8a14a4e3/PRN-bottleneck.png" alt="PRN-bottleneck"></li></ul><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul><li>更丰富的梯度传播路径<br><img src="/posts/8a14a4e3/grad.png"></li><li>网络结构的灵活性</li><li>轻量</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/edc10435.html">SparseNet</a></li><li><a href="http://blinging.xyz/posts/b956e9d5.html">CSPNet</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light-weight </tag>
            
            <tag> ICCV </tag>
            
            <tag> backbone </tag>
            
            <tag> gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPNN</title>
      <link href="posts/d3623899.html"/>
      <url>posts/d3623899.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Relation-Parsing-Neural-Network-for-Human-Object-Interaction-Detection》"><a href="#《Relation-Parsing-Neural-Network-for-Human-Object-Interaction-Detection》" class="headerlink" title="《Relation Parsing Neural Network for Human-Object Interaction Detection》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Detection_ICCV_2019_paper.pdf">Relation Parsing Neural Network for Human-Object Interaction Detection</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>目前的HOI方法从人和物体的特征以及他们的相对位置来预测人物交互，但是人实际是通过身体部件和物体进行交互的，身体部件应该得到更多的关注。</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>引入了身体部件特征</p></li><li><p>引入了(人-身体部件)注意力和(物体-身体部件)注意力</p></li></ul><p><img src="/posts/d3623899/RPNN.png"></p><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>《<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Haoshu_Fang_Pairwise_Body-Part_Attention_ECCV_2018_paper.pdf">Pairwise body-part attention for recognizing human-object interactions</a>》</li><li>《<a href="http://in.arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>》</li><li>《<a href="https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf">Convolutional networks on graphs for learning molecular fingerprints</a>》</li><li>《<a href="https://papers.nips.cc/paper/6212-diffusion-convolutional-neural-networks.pdf">Diffusion-convolutional neural networks</a>》</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Visual Cognition </tag>
            
            <tag> relationship </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThunderNet</title>
      <link href="posts/bd9092df.html"/>
      <url>posts/bd9092df.html</url>
      
        <content type="html"><![CDATA[<h3 id="《ThunderNet-Towards-Real-time-Generic-Object-Detection-on-Mobile-Devices》"><a href="#《ThunderNet-Towards-Real-time-Generic-Object-Detection-on-Mobile-Devices》" class="headerlink" title="《ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.pdf">ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>目前没有轻量级二阶段检测模型</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>backbone</p><ul><li>着眼运行效率和感受野，以ShuffleNet v2为基础进行修改，将3x3的DW Conv全部改成5x5的DW Conv</li></ul></li><li><p>detection head</p><ul><li><p>设计Context Enhancement Module以增大感受野</p><p><img src="/posts/bd9092df/CEM.jpg" alt="Fig. 1 CEM"></p></li><li><p>设计Spacial Attention Module改善用于PSRoI align的特征</p><p><img src="/posts/bd9092df/SAM.jpg" alt="Fig. 2 SAM"></p></li><li><p>用5x5DWConv+1x1Conv取代3x3Conv</p></li></ul></li></ul><p><img src="/posts/bd9092df/ThunderNet.jpg" alt="Fig. 3 网络结构"></p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> light-weight </tag>
            
            <tag> two-stage </tag>
            
            <tag> ICCV </tag>
            
            <tag> backbone </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TridentNet</title>
      <link href="posts/92fd9c72.html"/>
      <url>posts/92fd9c72.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Scale-Aware-Trident-Networks-for-Object-Detection》"><a href="#《Scale-Aware-Trident-Networks-for-Object-Detection》" class="headerlink" title="《Scale-Aware Trident Networks for Object Detection》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.pdf">Scale-Aware Trident Networks for Object Detection</a>》</h3><a id="more"></a><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li><p>通过对backbone网络的部分层设置不同的扩张率，探索了感受野对目标检测效果的影响</p><ul><li><p>感受野需要和目标尺度相匹配</p></li><li><p>有效感受野小于理论感受野</p></li><li><p>感受野需要在大目标和小目标之间找到平衡</p></li></ul></li><li><p>权重共享，扩张率不同的多条分支生成不同感受野的特征以应对不同尺度的目标</p><p><img src="/posts/92fd9c72/TridentNet.png"></p></li><li><p>类似SNIP，采用scale-aware的训练方法</p></li><li><p>可以使用主分支检测达到近似三分支的精度(扩展尺度范围为0到∞相当于放弃了scale-aware训练策略？)</p></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
            <tag> scale </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VrR-VG</title>
      <link href="posts/bdcfc59d.html"/>
      <url>posts/bdcfc59d.html</url>
      
        <content type="html"><![CDATA[<h3 id="《VrR-VG-Refocusing-Visually-Relevant-Relationships》"><a href="#《VrR-VG-Refocusing-Visually-Relevant-Relationships》" class="headerlink" title="《VrR-VG: Refocusing Visually-Relevant Relationships》"></a>《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.pdf">VrR-VG: Refocusing Visually-Relevant Relationships</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>现有的关系预测模型会拟合静态偏差（空间位置、固定搭配等可以从非视觉信息获取的结果）而不是学习视觉信息</li></ul><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul><li><p>神经网络有强大的从非视觉信息预测的能力</p></li><li><p>空间关系(on、in等)完全可以通过bbox坐标来预测</p></li><li><p>某些关系(wear、ride、has等)可以从语言先验或者统计分析来预测</p><p><img src="/posts/bdcfc59d/distribution2.png"></p></li><li><p>现有数据集有很多上述标签</p><p><img src="/posts/bdcfc59d/distribution1.png"></p></li></ul><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li><p>新的关系数据集<a href="http://vrr-vg.com/">VrR-VG</a></p></li><li><p>关系感知模型</p><ul><li><p>可以通过坐标、类别等信息来预测的关系是视觉无关的</p></li><li><p>Visual Discriminator: VD-Net通过坐标、类别、词汇等非视觉信息来预测关系</p></li><li><p>用VD-Net无法准确预测的关系标签来训练视觉感知模型并用于VQA等任务</p></li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Visual Cognition </tag>
            
            <tag> relationship </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientNet</title>
      <link href="posts/a8875d51.html"/>
      <url>posts/a8875d51.html</url>
      
        <content type="html"><![CDATA[<h3 id="《EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks》"><a href="#《EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks》" class="headerlink" title="《EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks》"></a>《<a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a>》</h3><a id="more"></a><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><p>基于以下发现，探索最优的宽度、深度、分辨率配置</p><ul><li>增加宽度、深度、分辨率都能带来精度提升，但边际效益递减</li><li>同样FLOPS下，不同宽度、深度、分辨率配置的神经网络性能有差异</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/a510d887.html">EfficientDet</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
            <tag> ICML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FishNet</title>
      <link href="posts/e04a0e51.html"/>
      <url>posts/e04a0e51.html</url>
      
        <content type="html"><![CDATA[<h3 id="《FishNet-A-Versatile-Backbone-for-Image-Region-and-Pixel-Level-Prediction》"><a href="#《FishNet-A-Versatile-Backbone-for-Image-Region-and-Pixel-Level-Prediction》" class="headerlink" title="《FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction》"></a>《<a href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf">FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>目前区域/像素级别的任务都在使用分类预训练网络作为backbone，而没有一个专门设计的backbone</li><li>目前的工作无法直接地将梯度反传到浅层</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>提出了FishNet结构（降采样-上采样-降采样结构）</p><p><img src="/posts/e04a0e51/FishNet.jpg" alt="FishNet"></p></li><li><p>对ResBlock进行修改，取消了identity mapping中的卷积，降采样/增加通道放在外部</p><p><img src="/posts/e04a0e51/resblock.jpg" alt="ResBlock"></p></li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>《Stacked Hourglass Networks for Human Pose Estimation》</li><li>《DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation》</li><li>《M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network》</li><li>《CBNet: A Novel Composite Backbone Network Architecture for Object Detection》</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fusion </tag>
            
            <tag> NeurIPS </tag>
            
            <tag> detection </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Imbalanced Attribute Classification using Visual Attention Aggregation</title>
      <link href="posts/9083591f.html"/>
      <url>posts/9083591f.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Deep-Imbalanced-Attribute-Classification-using-Visual-Attention-Aggregation》"><a href="#《Deep-Imbalanced-Attribute-Classification-using-Visual-Attention-Aggregation》" class="headerlink" title="《Deep Imbalanced Attribute Classification using Visual Attention Aggregation》"></a>《<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper.pdf">Deep Imbalanced Attribute Classification using Visual Attention Aggregation</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>现有方法忽略了空间和语义信息</li><li>属性分类中类别不均衡</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>每种属性分别引入了空间注意力机制</li><li>属性频率指数加权的focal loss</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> loss </tag>
            
            <tag> ECCV </tag>
            
            <tag> classification </tag>
            
            <tag> attribute </tag>
            
            <tag> attention </tag>
            
            <tag> Visual Cognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DetNet</title>
      <link href="posts/a562ac22.html"/>
      <url>posts/a562ac22.html</url>
      
        <content type="html"><![CDATA[<h3 id="《DetNet-Design-backbone-for-object-detection》"><a href="#《DetNet-Design-backbone-for-object-detection》" class="headerlink" title="《DetNet: Design backbone for object detection》"></a>《<a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Zeming_Li_DetNet_Design_Backbone_ECCV_2018_paper.pdf">DetNet: Design backbone for object detection</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>分类网络的设计准则（大幅降采样）不适合检测任务（定位）</li><li>RPN等网络虽然进行了优化，但仍有一些问题<ul><li>backbone的阶段（stage）数和检测的阶段数不同</li><li>由于深层特征谱的大步长，大目标定位不准确</li><li>小目标在降采样过程中消失</li></ul></li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>设计新的backbone</p><ul><li>stage数和检测相同</li><li>深层stage保持较高的分辨率</li><li>通过扩张残差块增大感受野</li></ul><p><img src="/posts/a562ac22/overview.png" alt="overview"><br><img src="/posts/a562ac22/detail.png" alt="detail"></p></li></ul><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><ul><li>仍需ImageNet预训练</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><ul><li><a href="https://github.com/zengarden/DetNet">https://github.com/zengarden/DetNet</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> backbone </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseNet</title>
      <link href="posts/edc10435.html"/>
      <url>posts/edc10435.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h3 id="《Sparsely-Aggregated-Convolutional-Networks》"><a href="#《Sparsely-Aggregated-Convolutional-Networks》" class="headerlink" title="《Sparsely Aggregated Convolutional Networks》"></a>《<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ligeng_Zhu_Sparsely_Aggregated_Convolutional_ECCV_2018_paper.pdf">Sparsely Aggregated Convolutional Networks</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>ResNet型密集聚合（+）：聚合后的特征不可分解，深层特征稀释或破坏浅层特征</li><li>DenseNet型密集聚合（concat）：参数以\(O(N^2)\)增加，消耗大量参数和运算来处理同一块特征</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>保留短梯度路径（skip connection）的优势，但避免类ResNet、DenseNet的稠密聚合</li><li>\(y_l=F_l(\bigotimes(y_{l-c^0},y_{l-c^1},y_{l-c^2}…y_{l-c^k}))\)</li></ul><p><img src="/posts/edc10435/SparseNet.png"></p><h3 id="相关："><a href="#相关：" class="headerlink" title="相关："></a>相关：</h3><ul><li><a href="http://blinging/posts/8a14a4e3.html">PRN</a></li><li><a href="http://blinging/posts/b956e9d5.html">CSPNet</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cascade R-CNN</title>
      <link href="posts/bd869b1e.html"/>
      <url>posts/bd869b1e.html</url>
      
        <content type="html"><![CDATA[<h3 id="《Cascade-R-CNN-Delving-into-High-Quality-Object-Detection》"><a href="#《Cascade-R-CNN-Delving-into-High-Quality-Object-Detection》" class="headerlink" title="《Cascade R-CNN: Delving into High Quality Object Detection》"></a>《<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf">Cascade R-CNN: Delving into High Quality Object Detection</a>》</h3><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>用低IoU阈值训练的检测器往往会给出带噪的检测结果，但是一味地提升训练时的IoU阈值也会使检测器性能下降。</li></ul><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul><li>随着IoU阈值提高，训练时的正样本数量急剧下降</li><li>训练时正样本与gt的IoU较高，推理时候选样本和gt的IoU可能没那么高，两者不匹配</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>级联回归，每一级一次上调IoU阈值并调整回归误差的均值方差。</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>《Object detection with discriminatively trained part-based models》</li><li>《Object detection via a multiregion and semantic segmentation-aware CNN model》</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> detection </tag>
            
            <tag> two-stage </tag>
            
            <tag> cascade </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MS-CNN</title>
      <link href="posts/a2ad476c.html"/>
      <url>posts/a2ad476c.html</url>
      
        <content type="html"><![CDATA[<h3 id="《A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection》"><a href="#《A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection》" class="headerlink" title="《A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection》"></a>《<a href="https://arxiv.org/pdf/1607.07155.pdf">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</a>》</h3><a id="more"></a><table><thead><tr><th align="center">问题</th><th align="center">方案</th></tr></thead><tbody><tr><td align="center">感受野和目标尺度不匹配问题</td><td align="center">多尺度生成proposal，每个特征谱预测特定尺度的目标</td></tr><tr><td align="center">对浅层的BP容易带来训练不稳定</td><td align="center">在conv4_3后接一个buffer conv层</td></tr><tr><td align="center"></td><td align="center">与gt的IoU小于0.2的proposal定为负样本</td></tr><tr><td align="center">深层特征对小目标不敏感</td><td align="center">conv4_3的输出上采样2x后作为RoI Pooling的输入</td></tr><tr><td align="center">上下文有利于分割和检测</td><td align="center">RoI的特征和其上下文同时做RoI Pooling并concat</td></tr></tbody></table><p><img src="/posts/a2ad476c/RPN.png" alt="Probosal sub-network"></p><p><img src="/posts/a2ad476c/DET.png" alt="Object detection sub-network"></p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
