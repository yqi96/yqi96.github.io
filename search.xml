<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Towards Open World Object Detection</title>
      <link href="/posts/49a43533.html"/>
      <url>/posts/49a43533.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><ol><li>open set learning——能识别unknown类的目标</li><li>incremental learning——能通过后续标注在不忘记旧类的前提下学到新类</li></ol><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/posts/49a43533/framework.png" alt="Fig. 1 整体框架"></p><h4 id="Contrastive-Clustering"><a href="#Contrastive-Clustering" class="headerlink" title="Contrastive Clustering"></a>Contrastive Clustering</h4><p>为了学到合适的特征空间（同类聚集，异类远离），添加contrastive loss：</p><script type="math/tex; mode=display">\begin{equation}\mathcal{L}_{cont}(f_c)=\sum_{i=0}^C l(f_c, p_i), where \\l(f_c, p_i)=\begin{cases}\mathcal{D}(f_c, p_i), & i=c\\\max\{0, \Delta-\mathcal{D}(f_c, p_i)\}, & otherwise\end{cases}\end{equation}</script><p>其中\(p_i\)为各类的特征均值（包括unknown），随训练更新，\(f_c\)为\(c\)类的特征，\(\mathcal{D}\)为欧氏距离，\(\Delta\)为常数。</p><h4 id="Auto-labelling-Unknowns-with-RPN"><a href="#Auto-labelling-Unknowns-with-RPN" class="headerlink" title="Auto-labelling Unknowns with RPN"></a>Auto-labelling Unknowns with RPN</h4><p>利用RPN所产生的false positive作为unknown类的特征用于上文\(p<em>i\)的更新以及\(\mathcal{L}</em>{cont}\)的计算。false positive根据置信度降序排序，并取top-1。</p><h4 id="Energy-Based-Unknown-Identifier"><a href="#Energy-Based-Unknown-Identifier" class="headerlink" title="Energy Based Unknown Identifier"></a>Energy Based Unknown Identifier</h4><p>用于推理阶段的unknown类决策。定义了一个能量函数：</p><script type="math/tex; mode=display">\begin{equation}E(f;g)=-T\log\sum_{i=1}^C\exp(\frac{g_i(f)}{T})\end{equation}</script><p>其中\(g_i(f)\)是第\(i\)类的logit，\(T\)是温度常数。</p><p>统计了known类和unknown类的\(E(f|g)\)取值分布，用韦伯分布来拟合，分别为\(\xi<em>{kn}(f)\)和\(\xi</em>{unk}(f)\)。用\(\xi<em>{kn}(f)&lt;\xi</em>{unk}(f)\)作为unknown类的判别条件。</p><p>能量函数的使用基于Contrastive Clustering已经令unknown类的特征远离known类的特征，这样用unknown类的特征计算的known类的logits相比known类的特征计算的known类的logits整体分布上有差异，呈现Fig. 2状。对两个分布函数的拟合完全没必要，只需要B点的横坐标即可。且实际只要A、B两点的横坐标接近，用什么分布拟合都无所谓。</p><p><img src="/posts/49a43533/dist.png" alt="Fig. 2 known和unknown的能量分布"></p><h4 id="Alleviating-Forgetting"><a href="#Alleviating-Forgetting" class="headerlink" title="Alleviating Forgetting"></a>Alleviating Forgetting</h4><p>看起来就是incremental learning常规的一套。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>开放世界目标检测任务的提出以及本文的方法意义不大。从实验设计上看，task2到task4不是few-shot learning，并不适用于online learning。而且全文除了用RPN来生成unknown的伪标签，都跟目标检测没什么关系。</p><p>反而是所谓的by-product，显式的open set learning有利于incremental learning，比较有意思。</p><blockquote><p>@inproceedings{owod,<br> title={Towards Open World Object Detection},<br> author={K J Joseph and Salman Khan and Fahad Shahbaz Khan and Vineeth N Balasubramanian},<br> booktitle={Conference on Computer Vision and Pattern Recognition},<br> year={2021},<br> pdf={<code>https://arxiv.org/pdf/2103.02603.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IC-Conv</title>
      <link href="/posts/9bf2951e.html"/>
      <url>/posts/9bf2951e.html</url>
      
        <content type="html"><![CDATA[<h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><p>讲的感受野的故事，结构上就是一个不同分支用不同扩张卷积的Inception块，配合网络结构搜索（剪枝）来确定具体参数。</p><p><img src="/posts/9bf2951e/IC-Conv.png" alt></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/yifan123/IC-Conv">https://github.com/yifan123/IC-Conv</a></p><blockquote><p>@article{liu2020inception,<br>title={Inception Convolution with Efficient Dilation Search},<br> author={Liu, Jie and Li, Chuming and Liang, Feng and Lin, Chen and Sun, Ming and Yan, Junjie and Ouyang, Wanli and Xu, Dong},<br> journal={arXiv preprint arXiv:2012.13587},<br> year={2020}<br> }</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> detection </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多卡训练时非tensor输入</title>
      <link href="/posts/5a33d4b6.html"/>
      <url>/posts/5a33d4b6.html</url>
      
        <content type="html"><![CDATA[<p>DataParallel类的forward中通过scatter方法把输入分配到各显卡，并行计算后通过gather方法进行整合。scatter只实现了对torch.Tensor的按batch维度切分，tuple、list、dict等类则进行递归，其他类型保持不变。</p><p>以list：[Tensor1, Tensor2]作为输入，2卡训练，scatter将Tensor1和Tensor2分别切分成2个：Tensor1_a、Tensor1_b、Tensor2_a、Tensor2_b，并把[Tensor1_a, Tensor2_a]作为GPU1的输入，[Tensor1_b, Tensor2_b]作为GPU2的输入。</p><p>以list：[ndarray1, ndarray2]作为输入，2卡训练时，由于该list内的元素都不是Tensor，因此GPU1和GPU2都接受完整的[ndarray1, ndarray2]作为输入。</p><p>Pytorch 1.7的scatter的具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scatter</span>(<span class="params">inputs, target_gpus, dim=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Slices tensors into approximately equal chunks and</span></span><br><span class="line"><span class="string">    distributes them across given GPUs. Duplicates</span></span><br><span class="line"><span class="string">    references to objects that are not tensors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scatter_map</span>(<span class="params">obj</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, torch.Tensor):</span><br><span class="line">            <span class="keyword">return</span> Scatter.apply(target_gpus, <span class="literal">None</span>, dim, obj)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj)))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>, <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj))))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">dict</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">type</span>(obj), <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj.items()))))</span><br><span class="line">        <span class="keyword">return</span> [obj <span class="keyword">for</span> targets <span class="keyword">in</span> target_gpus]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># After scatter_map is called, a scatter_map cell will exist. This cell</span></span><br><span class="line">    <span class="comment"># has a reference to the actual function scatter_map, which has references</span></span><br><span class="line">    <span class="comment"># to a closure that has a reference to the scatter_map cell (because the</span></span><br><span class="line">    <span class="comment"># fn is recursive). To avoid this reference cycle, we set the function to</span></span><br><span class="line">    <span class="comment"># None, clearing the cell</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = scatter_map(inputs)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        scatter_map = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>某些情况下可能需要以长度为batchsize的list作为输入。以2卡为例，希望把list的前半分配给GPU1，后半分配给GPU2，这是无法用DataParalle来实现的。采取策略为：list内的元素不用Tensor，可以用np.ndarray或其他类型，这样每张卡上的模型都会接受到完整的list。另外设置一个torch.Tensor类的index变量作为输入，scatter会将index切分成两份。在模型的forward中利用index来获取对应的list切片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>():</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x, index</span>):</span><br><span class="line">        x = x[<span class="built_in">int</span>(index[<span class="number">0</span>]) : <span class="built_in">int</span>(index[-<span class="number">1</span>]) + <span class="number">1</span>]</span><br><span class="line">        x = [torch.from_numpy(i).to(index.device) <span class="keyword">for</span> i <span class="keyword">in</span> inputs]</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">model = nn.DataParalle(Model())</span><br><span class="line">index = torch.tensor(<span class="built_in">list</span>(<span class="built_in">range</span>(batch_size))).to(device)</span><br><span class="line">x = [ndarray1, ndarray2, ..., ndarray16]</span><br><span class="line">out = model(x, index)</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataLoader无限循环</title>
      <link href="/posts/52f897dd.html"/>
      <url>/posts/52f897dd.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inf_gen</span>(<span class="params">dataloader</span>):</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> dataloader:</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPNN</title>
      <link href="/posts/d3623899.html"/>
      <url>/posts/d3623899.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>目前的HOI方法从人和物体的特征以及他们的相对位置来预测人物交互，但是人实际是通过身体部件和物体进行交互的，身体部件应该得到更多的关注。</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>引入了身体部件特征</p></li><li><p>引入了(人-身体部件)注意力和(物体-身体部件)注意力</p></li></ul><p><img src="/posts/d3623899/RPNN.png" alt></p><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>《<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Haoshu_Fang_Pairwise_Body-Part_Attention_ECCV_2018_paper.pdf">Pairwise body-part attention for recognizing human-object interactions</a>》</li><li>《<a href="http://in.arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>》</li><li>《<a href="https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf">Convolutional networks on graphs for learning molecular fingerprints</a>》</li><li>《<a href="https://papers.nips.cc/paper/6212-diffusion-convolutional-neural-networks.pdf">Diffusion-convolutional neural networks</a>》</li></ul><blockquote><p>@INPROCEEDINGS{prnn,<br> author={P. {Zhou} and M. {Chi}},<br> booktitle={International Conference on Computer Vision},<br> title={Relation Parsing Neural Network for Human-Object Interaction Detection},<br> year={2019},<br> pages={843-851},<br> pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Dete&gt;ction_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Visual Cognition </tag>
            
            <tag> relationship </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Softmax和Cross-entropy的数值稳定问题</title>
      <link href="/posts/6deaeeb1.html"/>
      <url>/posts/6deaeeb1.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } },  "HTML-CSS": { linebreaks: { automatic: true } },         SVG: { linebreaks: { automatic: true } }});</script><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>Softmax公式:</p><script type="math/tex; mode=display">\begin{equation}p_i=\frac{e^{l_i}}{\sum_\limits{j}e^{l_j}}\end{equation}</script><p>当logits太大时，指数求和部分会出现上溢，解决方案：</p><script type="math/tex; mode=display">\begin{equation}p_i=\frac{e^{l_i-l_{max}}}{\sum_\limits{j}e^{l_j-l_{max}}}\end{equation}</script><p>\(l<em>j-l</em>{max}\le0\)且必有至少一项为0，因此指数和不会出现上溢，分母不会出现下溢。</p><p>Cross-entropy公式：</p><script type="math/tex; mode=display">\begin{equation}xent=-\sum_\limits{i}y_i\ln p_i\end{equation}</script><p>若\(p_i\)接近0，则\(\ln p_i\)会发生下溢，解决方案：</p><p>将(2)代入(3),有</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}xent&=-\sum_\limits{i}y_i\ln {(\frac{e^{l_i-l_{max}}}{\sum_\limits{j}e^{l_j-l_{max}}})} \\\\&=-\sum_\limits{i}y_i(l_i-l_{max}-\ln{\sum_\limits{j}e^{l_j-l_{max}}})\\\\&=\sum_\limits{i}y_i(\ln{\sum_\limits{j}e^{l_j-l_{max}}}+l_{max}-l_i)\end{aligned}\end{equation}</script><h3 id="Pytorch简单实现"><a href="#Pytorch简单实现" class="headerlink" title="Pytorch简单实现"></a>Pytorch简单实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxCrossEntropyLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim=<span class="number">1</span>, mean=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SoftmaxCrossEntropyLoss, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.mean = mean</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logits, targets</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(targets.shape) == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># one-hot</span></span><br><span class="line">            ones = torch.eye(logits.shape[self.dim]).to(targets.device)</span><br><span class="line">            targets = ones.index_select(<span class="number">0</span>, targets)</span><br><span class="line">        </span><br><span class="line">        max_logit = logits.<span class="built_in">max</span>()</span><br><span class="line">        exp_sum = (logits - max_logit).exp().<span class="built_in">sum</span>(dim=self.dim, keepdim=<span class="literal">True</span>)</span><br><span class="line">        log_softmax = logits - max_logit - exp_sum.log()</span><br><span class="line"></span><br><span class="line">        xent = - (targets * log_softmax).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.mean:</span><br><span class="line">            xent /= logits.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> xent</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 安装配置</title>
      <link href="/posts/1b0927de.html"/>
      <url>/posts/1b0927de.html</url>
      
        <content type="html"><![CDATA[<h3 id="基础功能"><a href="#基础功能" class="headerlink" title="基础功能"></a>基础功能</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org</span><br><span class="line">npm install hexo-cli -g</span><br><span class="line"><span class="built_in">mkdir</span> blog &amp;&amp; <span class="built_in">cd</span> blog</span><br><span class="line">hexo init</span><br><span class="line">npm install hexo-server --save</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><h3 id="克隆source文件"><a href="#克隆source文件" class="headerlink" title="克隆source文件"></a>克隆source文件</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf <span class="built_in">source</span></span><br><span class="line"><span class="built_in">rm</span> -rf scaffolds</span><br><span class="line">git <span class="built_in">clone</span> git@github.com:yqi96/yqi96.github.io.git ./tmp</span><br><span class="line"><span class="built_in">cp</span> -r tmp/* . &amp;&amp; <span class="built_in">rm</span> -rf tmp</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</span></span><br><span class="line">npm install hexo-renderer-pug --save</span><br><span class="line">npm install hexo-renderer-sass-next --save</span><br></pre></td></tr></table></figure><h3 id="页面显示"><a href="#页面显示" class="headerlink" title="页面显示"></a>页面显示</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-archive --save</span><br><span class="line">npm install hexo-generator-category --save</span><br><span class="line">npm install hexo-generator-tag --save</span><br><span class="line">npm install hexo-generator-search --save</span><br><span class="line"><span class="comment"># npm install hexo-asset-image --save</span></span><br></pre></td></tr></table></figure><h3 id="唯一链接"><a href="#唯一链接" class="headerlink" title="唯一链接"></a>唯一链接</h3>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-abbrlink --save</span><br><span class="line">npm install https://github.com/foreveryang321/hexo-asset-image.git --save</span><br></pre></td></tr></table></figure><h3 id="备忘"><a href="#备忘" class="headerlink" title="备忘"></a>备忘</h3><ul><li>兼容<a href="https://github.com/Rozbo/hexo-abbrlink">hexo-abbrlink</a>的<a href="https://github.com/foreveryang321/hexo-asset-image">hexo-asset-image</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PRN</title>
      <link href="/posts/8a14a4e3.html"/>
      <url>/posts/8a14a4e3.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>之前的文献关注如何组合特征和如何使梯度得以传播，但是否可以通过得到更丰富的梯度组合来提高网络性能？</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>部分通道求和（下图中的特征谱按对应颜色的箭头进行正向传播）<br><img src="/posts/8a14a4e3/PRN-shrink.png" alt="PRN-shrink"><br><img src="/posts/8a14a4e3/PRN-expand.png" alt="PRN-expand"><br><img src="/posts/8a14a4e3/PRN-bottleneck.png" alt="PRN-bottleneck"></li></ul><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul><li>更丰富的梯度传播路径<br><img src="/posts/8a14a4e3/grad.png" alt></li><li>网络结构的灵活性</li><li>轻量</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/edc10435.html">SparseNet</a></li><li><a href="http://blinging.xyz/posts/b956e9d5.html">CSPNet</a></li></ul><blockquote><p>@INPROCEEDINGS{prn,<br> author={C. {Wang} and H. M. {Liao} and P. {Chen} and J. {Hsieh}},<br> booktitle={International Conference on Computer Vision Workshop},<br> title={Enriching Variety of Layer-Wise Learning Information by Gradient Combination},<br> year={2019},<br> pages={2477-2484},<br> pdf={<code>https://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Wang_Enriching_Variety_of_Layer-Wise_Learning_Information_by_Gradient_Combination_ICCVW_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light-weight </tag>
            
            <tag> ICCV </tag>
            
            <tag> backbone </tag>
            
            <tag> gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseNet</title>
      <link href="/posts/edc10435.html"/>
      <url>/posts/edc10435.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>ResNet型密集聚合（+）：聚合后的特征不可分解，深层特征稀释或破坏浅层特征</li><li>DenseNet型密集聚合（concat）：参数以\(O(N^2)\)增加，消耗大量参数和运算来处理同一块特征</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>保留短梯度路径（skip connection）的优势，但避免类ResNet、DenseNet的稠密聚合</li><li>\(y<em>l=F_l(\bigotimes(y</em>{l-c^0},y<em>{l-c^1},y</em>{l-c^2}…y_{l-c^k}))\)</li></ul><p><img src="/posts/edc10435/SparseNet.png" alt></p><h3 id="相关："><a href="#相关：" class="headerlink" title="相关："></a>相关：</h3><ul><li><a href="http://blinging/posts/8a14a4e3.html">PRN</a></li><li><a href="http://blinging/posts/b956e9d5.html">CSPNet</a></li></ul><blockquote><p>@InProceedings{sparsenet,<br> author={Zhu, Ligeng and Deng, Ruizhi and Maire, Michael and Deng, Zhiwei and Mori, Greg and Tan, Ping},<br> title={Sparsely Aggregated Convolutional Networks},<br> booktitle={European Conference on Computer Vision},<br> year={2018},<br> pages={192—208},<br> pdf={<code>https://openaccess.thecvf.com/content_ECCV_2018/papers/Ligeng_Zhu_Sparsely_Aggregated_Convolutional_ECCV_2018_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientDet</title>
      <link href="/posts/a510d887.html"/>
      <url>/posts/a510d887.html</url>
      
        <content type="html"><![CDATA[<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li>提出高效的特征融合模块BiFPN<ul><li>去除了只有一个输入的节点</li><li>添加额外的连接</li><li>特征求和部分添加可学习的权重参数</li><li>BiFPN多次叠加构成Neck<br><img src="/posts/a510d887/EfficientDet.png" alt="EfficientDet"></li></ul></li><li>用EfficientNet的思路探索了合适的Neck、Head和输入分辨率配置</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/a8875d51.html">EfficientNet</a></li></ul><blockquote><p>@INPROCEEDINGS{efficientdet,<br> author={M. {Tan} and R. {Pang} and Q. V. {Le}},<br> booktitle={Conference on Computer Vision and Pattern Recognition},<br> title={EfficientDet: Scalable and Efficient Object Detection},<br> year={2020},<br> pages={10778-10787},<br> pdf={<code>https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fusion </tag>
            
            <tag> CVPR </tag>
            
            <tag> detection </tag>
            
            <tag> neck </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSPNet</title>
      <link href="/posts/b956e9d5.html"/>
      <url>/posts/b956e9d5.html</url>
      
        <content type="html"><![CDATA[<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li>相比PRN，不但增加了梯度组合，还减少了运算</li><li>灵活性更强</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>按通道将特征谱分成两份，其中一份接各种模块后和另一份融合<br><img src="/posts/b956e9d5/CSP-dense.png" alt><br><img src="/posts/b956e9d5/CSP-res.png" alt></li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/edc10435.html">SparseNet</a></li><li><a href="http://blinging.xyz/posts/8a14a4e3.html">PRN</a></li></ul><blockquote><p>@INPROCEEDINGS{cspnet,<br> author={C. {Wang} and H. {Mark Liao} and Y. {Wu} and P. {Chen} and J. {Hsieh} and I. {Yeh}},<br> booktitle={Conference on Computer Vision and Pattern Recognition Workshops},<br> title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN},<br> year={2020},<br> pages={1571-1580},<br> pdf={<code>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientNet</title>
      <link href="/posts/a8875d51.html"/>
      <url>/posts/a8875d51.html</url>
      
        <content type="html"><![CDATA[<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><p>基于以下发现，探索最优的宽度、深度、分辨率配置</p><ul><li>增加宽度、深度、分辨率都能带来精度提升，但边际效益递减</li><li>同样FLOPS下，不同宽度、深度、分辨率配置的神经网络性能有差异</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li><a href="http://blinging.xyz/posts/a510d887.html">EfficientDet</a></li></ul><blockquote><p>@InProceedings{efficientnet,<br> title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},<br> author={Tan, Mingxing and Le, Quoc},<br> booktitle={Proceedings of the International Conference on Machine Learning},<br> pages={6105—6114},<br> year={2019},<br> volume={97},<br> series={Proceedings of Machine Learning Research},<br> pdf={<code>https://arxiv.org/pdf/1905.11946v4.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
            <tag> ICML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DAFS</title>
      <link href="/posts/102b1f4c.html"/>
      <url>/posts/102b1f4c.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>RefineDet的ODM预测时规则卷积的感受野和refine过的anchor位置不匹配</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>提出Dynamic Anchor Feature Selection(DAFS)，根据refine后的anchor动态地选择特征</li><li>改进RefineDet的TCB模块，双向聚合特征谱</li></ul><blockquote><p>@INPROCEEDINGS{dafs,<br> author={S. {Li} and L. {Yang} and J. {Huang} and X. {Hua} and L. {Zhang}},<br> booktitle={International Conference on Computer Vision},<br> title={Dynamic Anchor Feature Selection for Single-Shot Object Detection},<br> year={2019},<br> pages={6608-6617},<br> pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DetNet</title>
      <link href="/posts/a562ac22.html"/>
      <url>/posts/a562ac22.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>分类网络的设计准则（大幅降采样）不适合检测任务（定位）</li><li>RPN等网络虽然进行了优化，但仍有一些问题<ul><li>backbone的阶段（stage）数和检测的阶段数不同</li><li>由于深层特征谱的大步长，大目标定位不准确</li><li>小目标在降采样过程中消失</li></ul></li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>设计新的backbone</p><ul><li>stage数和检测相同</li><li>深层stage保持较高的分辨率</li><li>通过扩张残差块增大感受野</li></ul><p><img src="/posts/a562ac22/overview.png" alt="overview"><br><img src="/posts/a562ac22/detail.png" alt="detail"></p></li></ul><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><ul><li>仍需ImageNet预训练</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><ul><li><a href="https://github.com/zengarden/DetNet">https://github.com/zengarden/DetNet</a></li></ul><blockquote><p>@InProceedings{detnet,<br> author={Li, Zeming and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Deng, Yangdong and Sun, Jian},<br> title={DetNet: Design Backbone for Object Detection},<br> booktitle={European Conference on Computer Vision},<br> pages={339—354},<br> year={2018},<br> pdf={<code>https://eccv2018.org/openaccess/content_ECCV_2018/papers/Zeming_Li_DetNet_Design_Backbone_ECCV_2018_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> backbone </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MS-CNN</title>
      <link href="/posts/a2ad476c.html"/>
      <url>/posts/a2ad476c.html</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th style="text-align:center">问题</th><th style="text-align:center">方案</th></tr></thead><tbody><tr><td style="text-align:center">感受野和目标尺度不匹配问题</td><td style="text-align:center">多尺度生成proposal，每个特征谱预测特定尺度的目标</td></tr><tr><td style="text-align:center">对浅层的BP容易带来训练不稳定</td><td style="text-align:center">在conv4_3后接一个buffer conv层</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">与gt的IoU小于0.2的proposal定为负样本</td></tr><tr><td style="text-align:center">深层特征对小目标不敏感</td><td style="text-align:center">conv4_3的输出上采样2x后作为RoI Pooling的输入</td></tr><tr><td style="text-align:center">上下文有利于分割和检测</td><td style="text-align:center">RoI的特征和其上下文同时做RoI Pooling并concat</td></tr></tbody></table></div><p><img src="/posts/a2ad476c/RPN.png" alt="Probosal sub-network"></p><p><img src="/posts/a2ad476c/DET.png" alt="Object detection sub-network"></p><blockquote><p>@InProceedings{mscnn,<br> author={Cai, Zhaowei and Fan, Quanfu and Feris, Rogerio S. and Vasconcelos, Nuno},<br> title={A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection},<br> booktitle={European Conference on Computer Vision},<br> year={2016},<br> pages={354—370},<br> pdf={<code>https://arxiv.org/pdf/1607.07155.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LRFNet</title>
      <link href="/posts/127af3.html"/>
      <url>/posts/127af3.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>现有多数目标检测网络是从ImageNet预训练的分类网络微调的，但是检测和分类任务对平移的敏感性要求不同，预训练网络和检测任务之间有一定差异，用分类预训练网络不是最优的。但是从头开始训练检测网络需要大量时间。</li></ul><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li>用分类预训练网络做主体，外接一路从头训练的小网络，结合两者特征</li></ul><p><img src="/posts/127af3/LRF.png" alt></p><blockquote><p>@INPROCEEDINGS{lrfnet,<br> author={T. {Wang} and R. M. {Anwer} and H. {Cholakkal} and F. S. {Khan} and Y. {Pang} and L. {Shao}},<br> booktitle={International Conference on Computer Vision},<br> title={Learning Rich Features at High-Speed for Single-Shot Object Detection},<br> year={2019},<br> pages={1971-1980},<br> pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
            <tag> scratch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Imbalanced Attribute Classification using Visual Attention Aggregation</title>
      <link href="/posts/9083591f.html"/>
      <url>/posts/9083591f.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>现有方法忽略了空间和语义信息</li><li>属性分类中类别不均衡</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>每种属性分别引入了空间注意力机制</li><li>属性频率指数加权的focal loss</li></ul><blockquote><p>@InProceedings{diaa,<br> author={Sarafianos, Nikolaos and Xu, Xiang and Kakadiaris, Ioannis A.},<br> title={Deep Imbalanced Attribute Classification Using Visual Attention Aggregation},<br> booktitle={European Conference on Computer Vision},<br> year={2018},<br> pages={708—725},<br> pdf={<code>http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> loss </tag>
            
            <tag> ECCV </tag>
            
            <tag> classification </tag>
            
            <tag> attribute </tag>
            
            <tag> attention </tag>
            
            <tag> Visual Cognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VrR-VG</title>
      <link href="/posts/bdcfc59d.html"/>
      <url>/posts/bdcfc59d.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>现有的关系预测模型会拟合静态偏差（空间位置、固定搭配等可以从非视觉信息获取的结果）而不是学习视觉信息</li></ul><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul><li><p>神经网络有强大的从非视觉信息预测的能力</p></li><li><p>空间关系(on、in等)完全可以通过bbox坐标来预测</p></li><li><p>某些关系(wear、ride、has等)可以从语言先验或者统计分析来预测</p><p><img src="/posts/bdcfc59d/distribution2.png" alt></p></li><li><p>现有数据集有很多上述标签</p><p><img src="/posts/bdcfc59d/distribution1.png" alt></p></li></ul><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li><p>新的关系数据集<a href="http://vrr-vg.com/">VrR-VG</a></p></li><li><p>关系感知模型</p><ul><li><p>可以通过坐标、类别等信息来预测的关系是视觉无关的</p></li><li><p>Visual Discriminator: VD-Net通过坐标、类别、词汇等非视觉信息来预测关系</p></li><li>用VD-Net无法准确预测的关系标签来训练视觉感知模型并用于VQA等任务</li></ul></li></ul><blockquote><p>@INPROCEEDINGS{vrrvg,<br> author={Y. {Liang} and Y. {Bai} and W. {Zhang} and X. {Qian} and L. {Zhu} and T. {Mei}},<br> booktitle={International Conference on Computer Vision},<br> title={VrR-VG: Refocusing Visually-Relevant Relationships},<br> year={2019},<br> pages={10402-10411},<br> pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Visual Cognition </tag>
            
            <tag> relationship </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ClusDet</title>
      <link href="/posts/e77010ad.html"/>
      <url>/posts/e77010ad.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li><p>输入分辨率高</p></li><li><p>目标小</p></li><li><p>目标稀疏且非均匀分布</p></li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>检测目标聚集区域，并从该区域内检测，将结果与全局检测结合</li></ul><blockquote><p>@INPROCEEDINGS{clusdet,<br>author={F. {Yang} and H. {Fan} and P. {Chu} and E. {Blasch} and H. {Ling}},<br>booktitle={International Conference on Computer Vision},<br>title={Clustered Object Detection in Aerial Images},<br>year={2019},<br>pages={8310-8319},<br>pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
            <tag> aerial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThunderNet</title>
      <link href="/posts/bd9092df.html"/>
      <url>/posts/bd9092df.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>目前没有轻量级二阶段检测模型</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>backbone</p><ul><li>着眼运行效率和感受野，以ShuffleNet v2为基础进行修改，将3x3的DW Conv全部改成5x5的DW Conv</li></ul></li><li><p>detection head</p><ul><li><p>设计Context Enhancement Module以增大感受野</p><p><img src="/posts/bd9092df/CEM.jpg" alt="Fig. 1 CEM"></p></li><li><p>设计Spacial Attention Module改善用于PSRoI align的特征</p><p><img src="/posts/bd9092df/SAM.jpg" alt="Fig. 2 SAM"></p></li><li><p>用5x5DWConv+1x1Conv取代3x3Conv</p></li></ul></li></ul><p><img src="/posts/bd9092df/ThunderNet.jpg" alt="Fig. 3 网络结构"></p><blockquote><p>@INPROCEEDINGS{thundernet,<br> author={Z. {Qin} and Z. {Li} and Z. {Zhang} and Y. {Bao} and G. {Yu} and Y. {Peng} and J. {Sun}},<br> booktitle={International Conference on Computer Vision},<br> title={ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices},<br> year={2019},<br> pages={6717-6726},<br> pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> light-weight </tag>
            
            <tag> two-stage </tag>
            
            <tag> ICCV </tag>
            
            <tag> backbone </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CARAFE</title>
      <link href="/posts/ed54d5da.html"/>
      <url>/posts/ed54d5da.html</url>
      
        <content type="html"><![CDATA[<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><ul><li>解决检测、分割等任务中的上采样问题</li></ul><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul><li>大感受野</li><li>内容感知</li><li>轻量级</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p><img src="/posts/ed54d5da/CARAFE.png" alt></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><ul><li><a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></li></ul><blockquote><p>@INPROCEEDINGS{carafe,<br>author={J. {Wang} and K. {Chen} and R. {Xu} and Z. {Liu} and C. C. {Loy} and D. {Lin}},<br>booktitle={International Conference on Computer Vision},<br>title={CARAFE: Content-Aware ReAssembly of FEatures},<br>year={2019},<br>pages={3007-3016},<br>pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> light-weight </tag>
            
            <tag> ICCV </tag>
            
            <tag> upsample </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FishNet</title>
      <link href="/posts/e04a0e51.html"/>
      <url>/posts/e04a0e51.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>目前区域/像素级别的任务都在使用分类预训练网络作为backbone，而没有一个专门设计的backbone</li><li>目前的工作无法直接地将梯度反传到浅层</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li><p>提出了FishNet结构（降采样-上采样-降采样结构）</p><p><img src="/posts/e04a0e51/FishNet.jpg" alt="FishNet"></p></li><li><p>对ResBlock进行修改，取消了identity mapping中的卷积，降采样/增加通道放在外部</p><p><img src="/posts/e04a0e51/resblock.jpg" alt="ResBlock"></p></li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>《Stacked Hourglass Networks for Human Pose Estimation》</li><li>《DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation》</li><li>《M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network》</li><li>《CBNet: A Novel Composite Backbone Network Architecture for Object Detection》</li></ul><blockquote><p>@inproceedings{fishnet,<br> author = {Sun, Shuyang and Pang, Jiangmiao and Shi, Jianping and Yi, Shuai and Ouyang, Wanli},<br> booktitle = {Advances in Neural Information Processing Systems},<br> title={FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction},<br> volume = {31},<br> year = {2018},<br> pages = {754—764},<br> pdf={<code>https://proceedings.neurips.cc/paper/2018/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fusion </tag>
            
            <tag> NeurIPS </tag>
            
            <tag> detection </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cascade R-CNN</title>
      <link href="/posts/bd869b1e.html"/>
      <url>/posts/bd869b1e.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>用低IoU阈值训练的检测器往往会给出带噪的检测结果，但是一味地提升训练时的IoU阈值也会使检测器性能下降。</li></ul><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul><li>随着IoU阈值提高，训练时的正样本数量急剧下降</li><li>训练时正样本与gt的IoU较高，推理时候选样本和gt的IoU可能没那么高，两者不匹配</li></ul><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul><li>级联回归，每一级一次上调IoU阈值并调整回归误差的均值方差。</li></ul><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>《Object detection with discriminatively trained part-based models》</li><li>《Object detection via a multiregion and semantic segmentation-aware CNN model》</li></ul><blockquote><p>@INPROCEEDINGS{cascade,<br> author={Z. {Cai} and N. {Vasconcelos}},<br> booktitle={Conference on Computer Vision and Pattern Recognition},<br> title={Cascade R-CNN: Delving Into High Quality Object Detection},<br> year={2018},<br> pages={6154-6162},<br> pdf={<code>http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> detection </tag>
            
            <tag> two-stage </tag>
            
            <tag> cascade </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TridentNet</title>
      <link href="/posts/92fd9c72.html"/>
      <url>/posts/92fd9c72.html</url>
      
        <content type="html"><![CDATA[<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul><li><p>通过对backbone网络的部分层设置不同的扩张率，探索了感受野对目标检测效果的影响</p><ul><li><p>感受野需要和目标尺度相匹配</p></li><li><p>有效感受野小于理论感受野</p></li><li><p>感受野需要在大目标和小目标之间找到平衡</p></li></ul></li><li><p>权重共享，扩张率不同的多条分支生成不同感受野的特征以应对不同尺度的目标</p><p><img src="/posts/92fd9c72/TridentNet.png" alt></p></li><li><p>类似SNIP，采用scale-aware的训练方法</p></li><li><p>可以使用主分支检测达到近似三分支的精度(扩展尺度范围为0到∞相当于放弃了scale-aware训练策略？)</p></li></ul><blockquote><p>@INPROCEEDINGS{tridentnet,<br> author={Y. {Li} and Y. {Chen} and N. {Wang} and Z. {Zhang}},<br> booktitle={International Conference on Computer Vision},<br> title={Scale-Aware Trident Networks for Object Detection},<br> year={2019},<br> pages={6053-6062},<br> pdf={<code>http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.pdf</code>}}</p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> ICCV </tag>
            
            <tag> scale </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
